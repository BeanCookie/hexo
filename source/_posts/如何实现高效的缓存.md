---
title: 如何实现高效的缓存
date: 2020-07-24 09:42:04
categories:
- 缓存
tags: 
- Caffine
---

#### 为什么需要缓存

##### 硬件方面

在计算机体系结构中，CPU想要获取磁盘中的数据至少需要经过多级CPU缓存->内存->磁盘缓存，最终才能获取到磁盘中的数据。

![如何实现高效的本地缓存-01](https://beancookie.github.io/images/如何实现高效的本地缓存-01.png)

那为什么不能直接从本地磁盘中读取数据呢？

首先是不同存储介质的读写速度不匹配问题，CPU寄存器的读写响应时间都是在纳秒级别的甚至更快，而普通的机械硬盘或者速度更快的固态硬盘其读写速率要比CPU寄存器慢要几个数量级，如果CPU直接读取磁盘的话将会严重拖累CPU的处理速度。

那CPU缓存和内存速度很快，为什么不把所有数据都放到内存里面呢？

这是因为读写速度越快的介质单位价格就越贵，例如几百块钱就可以买到1T(1000GB)的机械硬盘，然而几十G的内存就要高达上千块钱。还有另外一个原因由于内存的物理介质的特性通常情况下都会在断电情况下丢失数据。

##### 算法方面

算法方面通常会引入时间复杂度这个概念来描述算法的快慢。

举个简单的例子：

> 现在我想读《三体》中末日之战时章北海喊出*前进三*的情节，最简单粗暴的方法就是从第一页开始一页一页的往后翻只到翻阅到指定的页数，用时间复杂度来描述呢就是O(n)
>
> 当然通常情况下读者并不会这么干，因为每本书的页数都是从小到大有序排列的，借助这一特性我们可以才有二分法进行查找。借用二分法的思想算法中又衍生出搜索树之类的数据结构，所有的这些努力都是为了将查找的时间复杂度降低到O(n)以下，上面所说的二分查找和搜索树就可以将平均时间复杂度降低到O(log(n))

#### 如何将时间复杂度降低到O(1)

还是以读书举例，如果每次都用二分查找的方式来寻找小说中的指定情节时我们依然会觉得很慢。有没有更快速的方法呢？如果我们借助书中的目录就可以一步到位的寻找到指定的章节。目录所起的作用就是记录一个映射关系将关键的情节与对应的页数一一对应，将我们的查找的时间复杂度降低到O(1)，在算法领域哈希表就像目录一样可以根据关键字快速定位到指定内容。

![如何实现高效的本地缓存-02](https://beancookie.github.io/images/如何实现高效的本地缓存-02.png)

![如何实现高效的本地缓存-03](https://beancookie.github.io/images/如何实现高效的本地缓存-03.png)

#### 如何尽量避免丢失热点数据

依然以看小说为例，大家都知道一万个人心中有一万个哈姆雷特，每个人对小说中的喜爱程度也不相同，有人喜欢逻辑，有人喜欢章北海，也有人喜欢雷迪亚兹。目录条目和计算机内存一样也不能无限制的加大，如何保证绝大多数的人喜欢的情节都可以在目录中体现，如何保证永远将最活跃的数据保存在内存中呢？

#### LRU与LFU

- **LRU**

  > Least Recently Used的缩写，即最近最少使用

  因为物理内存不可能无限扩容，所以在使用哈希表缓存数据时当数据量达到一定规模时需要淘汰部分数据，LRU实际上就是一种淘汰策略它假设越近被访问时数据，下次被访问的可能性越大。实现方式也并不复杂，只需要在哈希表的基础上使用一个双向链表记录元素的访问顺序，当数据被访问时将它移动到表尾，当需要淘汰数据时则从表头进行淘汰。

  ![如何实现高效的本地缓存-04](https://beancookie.github.io/images/如何实现高效的本地缓存-04.png)

  **LRU**的局限性正来自于它的淘汰策略，如果有个数据在 1 分钟访问了 1000次，再后 1 分钟没有访问这个数据，但是有其他的数据访问，就导致了我们这个热点数据被淘汰，所以**LRU**很难保障周期性的热点数据。

- **LFU**

  > Least Frequency Used的缩写，即最近最少频率使用

  为了优化**LRU**的局限性**LFU**利用额外的空间记录每个数据的使用频率，然后选出频率最低进行淘汰。这样就避免了**LRU**不能保障周期性热点数据的问题。但是鱼翅和熊掌不可兼得，比如有部新剧出来了，我们使用 LFU 给他缓存下来，这部新剧在这几天大概访问了几亿次，这个访问频率也在我们的 LFU 中记录了几亿次。但是新剧总会过气的，比如一个月之后这个新剧的前几集其实已经过气了，但是他的访问量的确是太高了，其他的电视剧根本无法淘汰这个新剧，所以在这种模式下是有局限性。

#### Caffine

在现有算法的局限性下，会导致缓存数据的命中率或多或少的受损，而命中略又是缓存的重要指标。HighScalability网站刊登了一篇文章，由前Google工程师发明的**Window TinyLfu**一种现代的缓存 。Caffine Cache就是基于此算法而研发。**Caffeine**因使用 **Window TinyLfu** 回收策略，提供了一个**近乎最佳的命中率**。

![如何实现高效的本地缓存-05](https://beancookie.github.io/images/如何实现高效的本地缓存-05.png)

TinyLFU维护了近期访问记录的频率信息，作为一个过滤器，当新记录来时，只有满足TinyLFU要求的记录才可以被插入缓存。如前所述，作为现代的缓存，它需要解决两个挑战

1. 一个是如何避免维护频率信息的高开销
2. 另一个是如何反应随时间变化的访问模式

首先来看前者，TinyLFU借助了数据流Sketching技术，Count-Min Sketch显然是解决这个问题的有效手段，它可以用小得多的空间存放频率信息，而保证很低的False Positive Rate。但考虑到第二个问题，就要复杂许多了，因为我们知道，任何Sketching数据结构如果要反应时间变化都是一件困难的事情，在Bloom Filter方面，我们可以有Timing Bloom Filter，但对于CMSketch来说，如何做到Timing CMSketch就不那么容易了。TinyLFU采用了一种基于滑动窗口的时间衰减设计机制，借助于一种简易的reset操作：每次添加一条记录到Sketch的时候，都会给一个计数器上加1，当计数器达到一个尺寸W的时候，把所有记录的Sketch数值都除以2，该reset操作可以起到衰减的作用 。

![如何实现高效的本地缓存-06](https://beancookie.github.io/images/如何实现高效的本地缓存-06.png)

![如何实现高效的本地缓存-07](https://beancookie.github.io/images/如何实现高效的本地缓存-07.png)

#### Jetcache

实际项目中**Caffeine**可以满足本地缓存，但是在分布式环境下最简单的方式还是使用**Redis**进行分布式的缓存，在实际项目中可以使用**Jetcache**提供的统一API和注解来简化缓存的使用，也可以必要时刻轻松转换为分布式的缓存。

#### Links

[http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html](http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html)

[https://github.com/ben-manes/caffeine](https://github.com/ben-manes/caffeine)

[https://github.com/alibaba/jetcache/wiki/Home_CN](https://github.com/alibaba/jetcache/wiki/Home_CN)